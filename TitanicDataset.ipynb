{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import and Study Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_path = \"/kaggle/input/titanic/train.csv\"\ntrain_df = pd.read_csv(train_path)\n\ntest_path = \"/kaggle/input/titanic/test.csv\"\ntest_df = pd.read_csv(test_path)\ntest_passenger_id = test_df.PassengerId.values\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train_df.columns:\n    print(col, \" No. of categories: \", len(train_df[col].value_counts()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# analyze influence of gender\nsurvived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\nax.set_title('Male')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-Processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nscaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = [train_df, test_df]\nlabels = train_df[['Survived']]\ntrain_df.drop('Survived', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(datasets)):\n    data = datasets[i]\n\n    # select columns \n    data = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n    \n    # encode categorical variables \n    Sex = pd.get_dummies(data.Sex)\n    Pclass = pd.get_dummies(data.Pclass, prefix=\"class\")\n    Embarked = pd.get_dummies(data.Embarked)\n\n    data = data.merge(Sex, how=\"left\", left_index=True, right_index=True).merge(Pclass, how=\"left\", left_index=True, right_index=True).merge(Embarked, how=\"left\", left_index=True, right_index=True)\n    data.drop(['Sex', 'Pclass', 'Embarked'], axis=1, inplace=True)\n        \n    # replace NAN values \n    data['Age'] = data['Age'].fillna(data['Age'].mean())\n    data['Fare'] = data['Fare'].fillna(data['Fare'].mean())\n    \n    # datatypes \n    data = data.astype({'Fare': 'int32'}) \n    data = data.astype({'Age': 'int32'}) \n    \n    feature_cols = [\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"female\",\"male\",\"class_1\",\"class_2\",\"class_3\",\"C\",\"Q\",\"S\"]\n    \n    # assign preprocessed dataset to train/test set\n    if i == 0:\n        train_df = data\n        scaler.fit(train_df[feature_cols])\n        x_train = scaler.transform(train_df[feature_cols])\n        y_train = labels.values\n    else: \n        test_df = data\n        x_test = scaler.transform(test_df[feature_cols])\n    \nprint(\"train and test set successfully pre-processed\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model\nTesting different models and comparing performances on the given dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results=[]\n\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nparam_grid = {\n            \"kernel\":[\"linear\", \"rbf\", \"poly\"],\n            \"C\":[4,5,6],\n            \"gamma\": [0.09, 0.1, 0.11]\n            }\n\ncv = GridSearchCV(estimator=SVC(), param_grid=param_grid, n_jobs=-1, cv =5)\ngrid_result = cv.fit(x_train, y_train)\n\n# summary\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstds = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with %r\" % (mean, stdev, param))\n\nresults.append(grid_result.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Nearest Neighbor Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nparam_grid = {\"n_neighbors\": [2,3,4,5,6,7,8,9,10]}\n\ncv = GridSearchCV(estimator = KNeighborsClassifier(), cv=5, param_grid=param_grid, n_jobs=-1)\ngrid_result = cv.fit(x_train, y_train)\n\n# summary\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstds = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with %r\" % (mean, stdev, param))\n\nresults.append(grid_result.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {\n            \"n_estimators\":[80, 90, 100],\n            \"max_depth\":[10,12],\n            \"min_samples_split\":[2,3],\n            \"max_features\":[0.6, 0.7]\n            }\n\ncv = GridSearchCV(estimator = RandomForestClassifier(), cv=5, param_grid=param_grid, n_jobs=-1)\ngrid_result = cv.fit(x_train, y_train)\n\n# summary\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstds = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with %r\" % (mean, stdev, param))\n    \nresults.append(grid_result.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nparam_grid = {\n            \"n_estimators\":[75,80,85],\n            \"learning_rate\":[0.005, 0.01, 0.015,],\n            \"max_depth\":[6,7],\n            \"min_samples_split\":[4,5],\n            \"max_features\":[0.4, 0.5]\n            }\n\ncv = GridSearchCV(estimator = GradientBoostingClassifier(), cv=5, param_grid=param_grid, n_jobs=-1)\ngrid_result = cv.fit(x_train, y_train)\n\n# summary\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstds = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with %r\" % (mean, stdev, param))\n\nresults.append(grid_result.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Artificial Neural Network\nFristly, a GridSearch will help to find ideal Parameter.\nSecondly, the Network is trainer and analyzed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.optimizers import *\nimport keras\nimport keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# define input dim\ninput_dim = x_train.shape[1]\noutput_dim = 1\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n\n# Function to create model, required for KerasRegressor\ndef create_model(activation, neurons, dropout, learning_rate):\n    \n    adam = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n\n    model = Sequential()\n    model.add(Dense(neurons, input_dim=input_dim))\n    model.add(Activation(activation))\n    model.add(Dropout(dropout))\n    model.add(Dense(neurons))\n    model.add(Activation(activation))\n    model.add(Dropout(dropout))\n    model.add(Dense(output_dim))\n    model.add(Activation(\"sigmoid\"))\n    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])    \n    return model\n\n# create model\ngrid_model = KerasClassifier(build_fn=create_model, epochs=20, verbose=1)\n\n# define the grid search parameters\nneurons = [8, 16]\ndropout= [0.1, 0.2, 0.3]\nactivation = ['tanh', 'relu']\nlearning_rate = [0.001, 0.0001]\n\n# define grid with parameters to be tuned\nparam_grid = dict(neurons=neurons,activation=activation, dropout=dropout, learning_rate=learning_rate)\n\n# instanciate GridSearchCV with defined scoring and cv\ngrid = GridSearchCV(estimator=grid_model, \n                    param_grid=param_grid, \n                    scoring='accuracy', \n                    cv=3)\n#fit grid model\ngrid_result = grid.fit(x_train, y_train)\n    \nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstds = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define input dim\ninput_dim = x_train.shape[1]\noutput_dim = 1\n\nadam = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(16, input_dim=input_dim))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(16))\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(output_dim))\nmodel.add(Activation(\"sigmoid\"))\n\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy', precision_m, recall_m, f1_m])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nmodel_history = model.fit(x_train, y_train, \n                            epochs=200, \n                            validation_split=0.2,\n                            shuffle=True,\n                            callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min')])\n\nprint(model_history.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n\naxs[0].plot(model_history.history['accuracy'], color=\"b\")\naxs[0].plot(model_history.history['val_accuracy'], color=\"black\")\naxs[0].set_xlabel('\\nEpoche', fontsize=14)\naxs[0].set_title('ACC\\n', fontsize=14)\naxs[0].legend(['Training', 'Validation'], loc='best')\n\naxs[1].plot(model_history.history['precision_m'], color=\"b\")\naxs[1].plot(model_history.history['val_precision_m'], color=\"black\")\naxs[1].set_xlabel('\\nEpoche', fontsize=14)\naxs[1].set_title('Precision\\n', fontsize=14)\naxs[1].legend(['Training', 'Validation'], loc='best')\n\naxs[2].plot(model_history.history['recall_m'], color=\"b\")\naxs[2].plot(model_history.history['val_recall_m'], color=\"black\")\naxs[2].set_xlabel('\\nEpoche', fontsize=14)\naxs[2].set_title('Recall\\n', fontsize=14)\naxs[2].legend(['Training', 'Validation'], loc='best')\naxs[2].yaxis.set_major_formatter(plt.FormatStrFormatter('%.2f'))\n\naxs[3].plot(model_history.history['f1_m'], color=\"b\")\naxs[3].plot(model_history.history['val_f1_m'], color=\"black\")\naxs[3].set_xlabel('\\nEpoche', fontsize=14)\naxs[3].set_title('\\nF1 Score\\n', fontsize=14)\naxs[3].legend(['Training', 'Validation'], loc='best')\naxs[3].yaxis.set_major_formatter(plt.FormatStrFormatter('%.2f'))\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_epochs = len(model_history.history['val_accuracy'])\nval_acc = model_history.history['val_accuracy']\nfinal_acc = val_acc[nb_epochs-1]\n\nresults.append(final_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance Analysis ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df = pd.DataFrame(data=results, columns=[\"Accuracy\"])\nresult_df.index=[\"SVC\", \"KNN\", \"Random Forest\", \"Gradient Boosting\", \"ANN\"]\nresult_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.bar(result_df.index, result_df.Accuracy, color=\"black\")\nplt.ylabel(\"ACC\\n\")\nplt.xticks(rotation=30)\nplt.ylim(0.7,1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Selection of best Model\n- Artificial Neural Network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# predictions\ny_pred = model.predict_classes(x_train)\n\ncm = confusion_matrix(y_train, y_pred)\nprint(\"Confusion Matrix Training Data: \\n\\n\", cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions on test data\ntest_predictions = model.predict_classes(x_test)\n\ndf1 = pd.DataFrame(test_passenger_id, index=None, columns=[\"PassengerId\"])\ndf2 = pd.DataFrame(data=test_predictions, columns=[\"Survived\"])\nfinal_data = df1.merge(df2, how=\"left\", left_index=True, right_index=True)\nfinal_data.to_csv('test.csv', header=True, index=False)\nfinal_data","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}